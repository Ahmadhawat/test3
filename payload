Sweet — here’s a drop-in builder that uses the strict system prompt by default and still enforces top-k + budget.

import textwrap
from typing import Dict, List, Tuple

# --- Strict system prompt: short answer, single best reference ---
STRICT_SYSTEM_PROMPT = """\
Du bist ein technischer Assistent für card_1 (CAD/BIM für Vermessung & Infrastruktur).

Regeln für deine Antwort:
1. Lies die Nutzerfrage und die bereitgestellten Quellen sorgfältig.
2. Wähle die EINE Quelle aus den Top-10, die am besten zur Frage passt.
3. Formuliere eine kurze, fachlich korrekte Antwort (max. 3–4 Sätze).
4. Gib IMMER die Referenznummer der gewählten Quelle in eckigen Klammern an (z. B. [3]).
5. Wenn keine Quelle die Frage beantworten kann, antworte „Keine passende Information gefunden.“ und nenne die nächste am ehesten relevante Referenznummer.
6. Füge am Ende eine Liste „Sources:“ an, die nur die verwendete Quelle zeigt, im Format:
   Sources:
   [n] <Dateipfad oder Chunkname>
Wichtig: Keine zusätzlichen Erklärungen, keine Quellen ohne Nummer, keine erfundenen Informationen.
"""

def trim_sources_to_budget(loaded_files: Dict[str, str],
                           top_k: int = 10,
                           total_budget_chars: int = 80_000,
                           min_per_src: int = 1500) -> List[Tuple[int, str, str]]:
    """
    Keep only top_k items (preserving order) and trim each to fit a total char budget.
    Returns [(index, path, snippet)] where index starts at 1.
    """
    items = list(loaded_files.items())[:top_k]
    if not items:
        return []
    per_src = max(min_per_src, total_budget_chars // len(items))
    return [(i, path, (text or "")[:per_src]) for i, (path, text) in enumerate(items, start=1)]

def build_payload_strict_best_one(query: str,
                                  loaded_files: Dict[str, str],
                                  model_name: str,
                                  *,
                                  system_prompt: str = STRICT_SYSTEM_PROMPT,
                                  top_k: int = 10,
                                  total_budget_chars: int = 80_000,
                                  temperature: float = 0.2,
                                  max_tokens: int = 512) -> dict:
    """
    Builds a ChatCompletion payload that:
      - injects the strict system prompt,
      - limits context to top_k sources and a total char budget,
      - asks for a short answer with exactly one reference.
    """
    limited = trim_sources_to_budget(
        loaded_files=loaded_files,
        top_k=top_k,
        total_budget_chars=total_budget_chars
    )

    evidence_blocks = []
    for idx, path, snippet in limited:
        block = textwrap.dedent(f"""\
        [{idx}] {path}
        --- BEGIN ---
        {snippet}
        --- END ---""").strip()
        evidence_blocks.append(block)

    evidence_block = "\n\n".join(evidence_blocks)

    user_prompt = f"""Frage: {query}

Nutze NUR die untenstehenden Quellen. Wähle die beste einzelne Quelle und zitiere sie mit [n].
Wenn nicht beantwortbar, gib das an und nenne die nächstbeste Referenznummer.

Quellen:
{evidence_block}
"""

    return {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

Example

# loaded_files = load_files_from_json("workspace/hybrid_results.json")  # your loader result
payload = build_payload_strict_best_one(
    query="Wie kann ich Querprofillinien bis zu einer bestimmten Station plotten?",
    loaded_files=loaded_files,
    model_name="llama3.3:70b",
    top_k=10,
    total_budget_chars=80_000,  # ~20k tokens evidence cap
)
# requests.post(API_URL, headers={"Authorization": f"Bearer {API_KEY}", "Content-Type":"application/json"}, json=payload)

This will prompt the model to return a short answer and exactly one reference like [3], plus a Sources block listing only that source.


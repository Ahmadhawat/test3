Got it ‚úÖ ‚Äî you already have a dict like:

{
   "C:\\...\\profile_0001.txt": ".... file contents ....",
   "C:\\...\\profdemo_bahn.txt": ".... file contents ....",
   ...
}

Now you want a function that:

1. Takes this dict (from your loader).


2. Builds a prompt with the query and the evidence (with source numbers).


3. Returns the ready-to-send payload for your LLM API call.



Here‚Äôs a clean implementation:

import textwrap

def build_rag_payload(query: str,
                      loaded_files: dict[str, str],
                      model_name: str,
                      system_prompt: str,
                      max_ctx_chars: int = 80_000,
                      temperature: float = 0.2,
                      max_tokens: int = 800) -> dict:
    """
    Baut den Payload f√ºr die RAG-Generierung:
    - query: die Nutzerfrage
    - loaded_files: {path: content}
    - system_prompt: Anweisung f√ºr das LLM
    - Beschr√§nkt den Gesamttext auf max_ctx_chars Zeichen
    """

    # --- trim context budget ---
    per_src = max(1500, max_ctx_chars // max(1, len(loaded_files)))
    blocks = []
    for i, (path, text) in enumerate(loaded_files.items(), start=1):
        snippet = (text or "")[:per_src]
        block = textwrap.dedent(f"""\
        [{i}] {path}
        --- BEGIN ---
        {snippet}
        --- END ---
        """).strip()
        blocks.append(block)

    evidence_block = "\n\n".join(blocks)

    # --- user prompt ---
    user_prompt = f"""\
Frage: {query}

Du erh√§ltst bis zu {len(loaded_files)} Quellen mit Inhalten.
Nutze NUR diese Quellen f√ºr deine Antwort und zitiere sie mit [n].
Wenn Infos fehlen, sage das offen.

Quellen:
{evidence_block}
"""

    # --- payload ---
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    return payload


---

Example usage

SYSTEM_PROMPT = """\
Du bist ein technischer Assistent f√ºr card_1.
Beantworte Fragen pr√§zise, fachlich korrekt, mit Inline-Referenzen [n].
Am Ende eine Liste "Sources:" mit den verwendeten Quellen.
"""

files_dict = load_files_from_json("workspace/hybrid_results.json")
payload = build_rag_payload(
    query="Wie kann ich Querprofillinien bis zu einer bestimmten Station plotten?",
    loaded_files=files_dict,
    model_name="llama3.3:70b",
    system_prompt=SYSTEM_PROMPT
)

print(payload.keys())


---

üëâ Do you also want me to add a post-processor that automatically appends a Sources: list based on the [n] references the model used?





You‚Äôre right ‚Äî I didn‚Äôt wire the limit into the payload builder. Here‚Äôs a version that uses a limit function to cap both the number of files (top-k) and the total context budget.

from typing import Dict, List, Tuple
import textwrap

def trim_sources_to_budget(loaded_files: Dict[str, str],
                           top_k: int = 10,
                           total_budget_chars: int = 80_000,
                           min_per_src: int = 1500) -> List[Tuple[int, str, str]]:
    """
    Keep only the first top_k files (order is preserved from your retrieval),
    and trim each file so the *total* evidence stays within total_budget_chars.
    Returns a list of (index, path, snippet).
    """
    items = list(loaded_files.items())[:top_k]
    if not items:
        return []

    per_src = max(min_per_src, total_budget_chars // len(items))
    limited = []
    for i, (path, text) in enumerate(items, start=1):
        snippet = (text or "")[:per_src]
        limited.append((i, path, snippet))
    return limited


def build_rag_payload_with_limit(query: str,
                                 loaded_files: Dict[str, str],
                                 model_name: str,
                                 system_prompt: str,
                                 *,
                                 top_k: int = 10,
                                 total_budget_chars: int = 80_000,
                                 temperature: float = 0.2,
                                 max_tokens: int = 800) -> dict:
    """
    Builds a ready-to-send payload for your LLM, using the limiter above.
    """
    limited_sources = trim_sources_to_budget(
        loaded_files=loaded_files,
        top_k=top_k,
        total_budget_chars=total_budget_chars,
    )

    evidence_blocks = []
    for idx, path, snippet in limited_sources:
        block = textwrap.dedent(f"""\
        [{idx}] {path}
        --- BEGIN ---
        {snippet}
        --- END ---""").strip()
        evidence_blocks.append(block)

    evidence_block = "\n\n".join(evidence_blocks)

    user_prompt = f"""Frage: {query}

Nutze NUR die folgenden Quellen und zitiere sie mit [n]. Wenn Informationen fehlen,
sage das offen.

Quellen:
{evidence_block}
"""

    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    return payload

Use it like this:

files_dict = load_files_from_json("workspace/hybrid_results.json")  # your loader
payload = build_rag_payload_with_limit(
    query="Wie kann ich Querprofillinien bis zu einer bestimmten Station plotten?",
    loaded_files=files_dict,
    model_name="llama3.3:70b",
    system_prompt="Du bist ein technischer Assistent f√ºr card_1 ...",
    top_k=10,
    total_budget_chars=80_000,  # ~20k tokens
)

This guarantees you never exceed your context window: it keeps only top-k files and trims each to fit a global evidence budget.


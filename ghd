Got it—your BM25 is running, but the ranking is being skewed. Your query is German, and if the index was built with the default (English) analyzer, most signal words (“querprofillinien”, “station”, “plotten”) aren’t stemmed, and German stop-words (“wie, kann, ich, bis, zu, einer…”) aren’t removed. With an OR parser this can surface the wrong chunks.

You have two solid fixes:


---

1) Immediate fix (no reindex): build a smarter OR query

Filter out ultra-common tokens (by DF), keep informative ones, and add a phrase boost. Drop this directly into your BM25 block.

# -------- BM25 / Whoosh search --------
from whoosh import scoring
from whoosh.query import Or, Term, Phrase, Every

wix = whoosh_index.open_dir(str(lroot))
bm25 = scoring.BM25F(B=0.75, K1=1.5)

def _build_bm25_query(wix, searcher, field: str, text: str,
                      min_token_len: int = 3,
                      max_df_ratio: float = 0.40,     # drop tokens appearing in >40% of docs
                      phrase_boost: float = 2.0):
    analyzer = wix.schema[field].analyzer
    toks = [t.text for t in analyzer(text)]

    N = max(1, searcher.doc_count_all())
    good = []
    for tok in toks:
        if len(tok) < min_token_len:
            continue
        df = searcher.doc_frequency(field, tok) or 0
        if df / N <= max_df_ratio:
            good.append(tok)

    # fallback if everything got filtered
    if not good:
        good = [tok for tok in toks if len(tok) >= min_token_len] or toks

    # OR query over informative tokens
    clauses = [Term(field, tok) for tok in good]
    q = Or(clauses)

    # add a boosted phrase if 2+ tokens (helps exact multi-term matches)
    if len(good) >= 2:
        ph = Phrase(field, good)
        ph.boost = phrase_boost
        q = Or([q, ph])
    return q, good

with wix.searcher(weighting=bm25) as searcher:
    try:
        q, used_tokens = _build_bm25_query(wix, searcher, "text", qtext)
    except Exception:
        q, used_tokens = Every(), []

    hits = searcher.search(q, limit=k_lex)
    lex_scores: Dict[int, float] = {int(h["doc_id"]): float(h.score) for h in hits}

If you log used_tokens, you’ll see it keeps “querprofillinien”, “station”, “plotten”, and quietly drops extremely common noise.

Optional debug (very helpful!):

log.info("BM25 tokens used: %s", used_tokens)
for h in hits[:5]:
    log.info("hit doc_id=%s score=%.3f matched=%s",
             h.get("doc_id"), h.score, h.matched_terms())


---

2) Proper fix (recommended): reindex with a German analyzer

When you can rebuild the Whoosh index, use a German pipeline so stop-words/stemming are right.

from whoosh.fields import Schema, TEXT, ID, NUMERIC
from whoosh import index
from whoosh.analysis import RegexTokenizer, LowercaseFilter, StopFilter, StemFilter, CharsetFilter
from whoosh.support.charset import accent_map

de_analyzer = (RegexTokenizer()
               | LowercaseFilter()
               | CharsetFilter(accent_map)      # fold ä/ö/ü/ß
               | StopFilter(lang="de")
               | StemFilter(lang="de"))         # German stemming

schema = Schema(
    doc_id=NUMERIC(stored=True, unique=True),
    path=ID(stored=True),
    text=TEXT(stored=False, analyzer=de_analyzer, phrase=True),
)

Rebuild the index with the same doc_id values you use in FAISS/metadata so fusion lines up.


---

Why your current results look “off”

English analyzer + German query ⇒ noisy tokens aren’t removed; informative ones aren’t stemmed/matched.

OR parsing then happily returns chunks that share common function words.

BM25 still penalizes common terms via IDF, but without German stop-word removal it’s not enough in many corpora.


The query-builder in (1) fixes this today by:

dropping tokens with very high document frequency,

boosting the full phrase when present.


Then, when convenient, rebuild with (2) for clean scoring without the extra query logic.


Absolutely—here’s a drop-in replacement for your class that removes Whoosh and uses rank_bm25 for the lexical branch while keeping your FAISS vector search + fusion logic intact.

It:

builds/loads a cached BM25 index (pickle) so you don’t pay the cost every query,

tokenizes with a simple regex + lowercase + optional stopwords,

aligns BM25 docs with your FAISS doc_id using vector_metadata.json,

keeps your fusion (minmax or rrf) exactly as before.



---

src/steps/hybrid_search_impls/CHybridSearchV1.py (BM25 + FAISS, no Whoosh)

from __future__ import annotations

from pathlib import Path
from typing import Dict, List, Tuple
import json
import os
import re
import pickle

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi

from src.steps.hybrid_search_impls.CHybridSearchA5 import hybridSearch
from src.pipeline.Context import Context
from src.utils.logging import get_logger

# -----------------------------
# Utilities
# -----------------------------

def l2_normalize(v: np.ndarray) -> np.ndarray:
    v = v.astype("float32")
    n = np.linalg.norm(v, axis=-1, keepdims=True)
    n = np.maximum(n, 1e-12)
    return v / n

def _is_ivf(idx) -> bool:
    return isinstance(
        idx,
        (faiss.IndexIVFFlat, faiss.IndexIVFPQ, faiss.IndexIVFScalarQuantizer),
    )

def _minmax(scores: Dict[int, float]) -> Dict[int, float]:
    if not scores:
        return {}
    vals = list(scores.values())
    lo, hi = min(vals), max(vals)
    if hi - lo <= 1e-12:
        return {k: 1.0 for k in scores}
    return {k: (v - lo) / (hi - lo) for k, v in scores.items()}

def _rff(rank: int, k: int = 60) -> float:
    return 1.0 / (k + rank)

_CHUNK_SUFFIX_RE = re.compile(r"_chunk\d{4}\.txt$", re.IGNORECASE)
_TOKEN_RE = re.compile(r"\b\w+\b", flags=re.UNICODE)

def chunk_to_real_path(chunk_filename: str, source_html_root: str | None = None) -> str:
    rel = chunk_filename.replace("\\", "/")
    rel = _CHUNK_SUFFIX_RE.sub(".htm", rel)
    rel = rel.replace("/", os.sep)
    return os.path.join(source_html_root, rel) if source_html_root else rel

def chunk_to_real_txt(chunk_filename: str, source_html_root: str | None = None) -> str:
    rel = chunk_filename.replace("\\", "/")
    rel = _CHUNK_SUFFIX_RE.sub(".txt", rel)
    rel = rel.replace("/", os.sep)
    return os.path.join(source_html_root, rel) if source_html_root else rel

def simple_tokenize(text: str, stopwords: set[str] | None = None) -> List[str]:
    toks = [t.lower() for t in _TOKEN_RE.findall(text)]
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return toks

# -----------------------------
# Hybrid search implementation (FAISS + BM25Okapi)
# -----------------------------

class CHybridSearchV1(hybridSearch):
    name = "chybridsearchv1"

    def _load_meta_id2name(self, meta_path: Path) -> Dict[int, str]:
        raw_meta = json.loads(meta_path.read_text(encoding="utf-8"))
        id2name: Dict[int, str] = {}
        if isinstance(raw_meta, dict) and "id_to_name" in raw_meta:
            id2name = {int(k): str(v) for k, v in raw_meta["id_to_name"].items()}
        elif isinstance(raw_meta, dict):
            try:
                id2name = {int(k): str(v) for k, v in raw_meta.items()}
            except Exception:
                pass
        elif isinstance(raw_meta, list):
            for m in raw_meta:
                if not isinstance(m, dict):
                    continue
                did = int(m.get("id"))
                name = (
                    m.get("chunkname")
                    or m.get("filename")
                    or m.get("file")
                    or m.get("name")
                    or ""
                )
                id2name[did] = str(name)
        return id2name

    def _build_or_load_bm25(
        self,
        log,
        id2name: Dict[int, str],
        source_txt_root: str,
        cache_file: Path,
        stopwords: set[str] | None,
    ) -> Tuple[BM25Okapi, List[int]]:
        """
        Returns (bm25, docid_list). docid_list[pos] = doc_id used by FAISS/meta.
        Cache file stores a pickle: {"bm25": bm25, "docid_list": docid_list}
        """
        if cache_file.exists():
            try:
                with cache_file.open("rb") as f:
                    data = pickle.load(f)
                bm25: BM25Okapi = data["bm25"]
                docid_list: List[int] = data["docid_list"]
                log.info("Loaded BM25 cache from %s (%d docs)", cache_file, len(docid_list))
                return bm25, docid_list
            except Exception as e:
                log.warning("Failed to load BM25 cache (%s). Rebuilding...", e)

        # Build from scratch
        docid_list: List[int] = []
        tokenized_docs: List[List[str]] = []
        missing, empty = 0, 0

        # Use a stable order to keep mapping reproducible
        for did in sorted(id2name.keys()):
            chunkname = id2name[did]
            txt_path = chunk_to_real_txt(chunkname, source_html_root=source_txt_root)
            try:
                txt = Path(txt_path).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                missing += 1
                continue
            tokens = simple_tokenize(txt, stopwords=stopwords)
            if not tokens:
                empty += 1
                continue
            docid_list.append(did)
            tokenized_docs.append(tokens)

        if not tokenized_docs:
            raise RuntimeError("No tokenized documents available for BM25.")

        bm25 = BM25Okapi(tokenized_docs)

        # persist
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with cache_file.open("wb") as f:
            pickle.dump({"bm25": bm25, "docid_list": docid_list}, f, protocol=pickle.HIGHEST_PROTOCOL)

        log.info(
            "Built BM25 index: %d docs (missing=%d, empty=%d). Cached at %s",
            len(docid_list), missing, empty, cache_file,
        )
        return bm25, docid_list

    def run(self, ctx: Context) -> None:
        log = get_logger(self.name)

        # -------- Paths & config --------
        vroot = Path(ctx.cfg["paths"]["vector_dataset_dir"])
        index_path = vroot / "vector_index.faiss"
        meta_path = vroot / "vector_metadata.json"

        if not index_path.exists() or not meta_path.exists():
            log.error("Missing FAISS index or metadata in %s", vroot)
            return

        qcfg = ctx.cfg.get("query", {}) or {}
        fcfg = ctx.cfg.get("fusion", {}) or {}
        ecfg = ctx.cfg.get("embedding", {}) or {}
        bcfg = ctx.cfg.get("bm25", {}) or {}

        k = int(qcfg.get("k", 10))
        k_vec = int(qcfg.get("k_vec", 30))
        k_lex = int(qcfg.get("k_lex", 30))
        method = str(fcfg.get("method", "minmax")).lower()  # "minmax" | "rrf"
        alpha = float(fcfg.get("alpha", 0.5))

        qtext = ctx.artifacts.get("corrected_query") or qcfg.get("text")
        if not qtext:
            log.error("No query text provided. Set artifacts['query_text'] or cfg.query.text")
            return

        # -------- Embedder (local) --------
        model_name = ecfg.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
        cache_folder = ecfg.get("cache_folder", "./data/models/st")
        embedder = SentenceTransformer(model_name, cache_folder=cache_folder)

        e5 = ctx.cfg.get("e5_prefixes", {}) or {}
        use_e5 = bool(e5.get("use", False))
        qprefix = e5.get("query_prefix", "query: ")

        # -------- FAISS search --------
        index = faiss.read_index(str(index_path))
        if _is_ivf(index):
            nprobe = int(ctx.cfg.get("faiss", {}).get("nprobe", 32))
            try:
                faiss.ParameterSpace().set_index_parameter(index, "nprobe", nprobe)
            except Exception:
                try:
                    index.nprobe = nprobe  # type: ignore[attr-defined]
                except Exception:
                    pass

        q_in = f"{qprefix}{qtext}" if use_e5 else qtext
        qv = embedder.encode([q_in], show_progress_bar=False)
        qv = l2_normalize(np.asarray(qv, dtype="float32"))

        D, I = index.search(qv, k_vec)
        vec_scores: Dict[int, float] = {}
        for doc_id, score in zip(I[0].tolist(), D[0].tolist()):
            if doc_id != -1:
                vec_scores[int(doc_id)] = float(score)

        # -------- BM25 (rank_bm25) lexical branch --------
        # Roots visible in your original code; keep or move to cfg
        source_html_root = r"C:\Entwicklung\IAAEntwicklung\KI1\CIH\Help"
        source_txt_root = r"C:\Entwicklung\IAAEntwicklung\KI1\upload\Daten\workspace\txt_files"

        # Stopwords: provide in cfg.bm25.stopwords or use a small German default
        stopwords_cfg = set(bcfg.get("stopwords", []))
        if not stopwords_cfg and bcfg.get("use_german_defaults", True):
            stopwords_cfg = {
                "und","oder","aber","wie","wo","was","wann","warum","ist","sind","ein","eine","einer",
                "der","die","das","den","dem","des","zu","mit","ohne","auf","im","in","am","an","bis",
                "nur","auch","ich","du","er","sie","es","wir","ihr","sie","kann","können","kannst",
                "für","von","vom","beim","zum","zur"
            }

        bm25_cache = Path(ctx.cfg.get("paths", {}).get("bm25_cache_file", "./data/workspace/bm25_cache.pkl"))

        id2name = self._load_meta_id2name(meta_path)
        try:
            bm25, docid_list = self._build_or_load_bm25(
                log=log,
                id2name=id2name,
                source_txt_root=source_txt_root,
                cache_file=bm25_cache,
                stopwords=stopwords_cfg,
            )
        except Exception as e:
            log.error("BM25 build/load failed: %s", e)
            return

        q_tokens = simple_tokenize(qtext, stopwords=stopwords_cfg)
        if not q_tokens:
            log.warning("Query produced no tokens for BM25; lexical branch will be empty.")
            lex_scores: Dict[int, float] = {}
        else:
            scores = bm25.get_scores(q_tokens)  # len == len(docid_list)
            # Top-k lexical by BM25
            topk_idx = np.argpartition(-scores, kth=min(k_lex, len(scores)-1))[:k_lex]
            # Build {doc_id: score}
            lex_scores = {int(docid_list[i]): float(scores[i]) for i in topk_idx}

        # -------- Fusion --------
        all_ids = set(vec_scores) | set(lex_scores)
        fused_pairs: List[Tuple[int, float]] = []

        if method == "rrf":
            vec_sorted = sorted(vec_scores.items(), key=lambda x: x[1], reverse=True)
            lex_sorted = sorted(lex_scores.items(), key=lambda x: x[1], reverse=True)
            vec_rank = {doc_id: r for r, (doc_id, _) in enumerate(vec_sorted, start=1)}
            lex_rank = {doc_id: r for r, (doc_id, _) in enumerate(lex_sorted, start=1)}
            for did in all_ids:
                s = _rff(vec_rank.get(did, 10**9)) + _rff(lex_rank.get(did, 10**9))
                fused_pairs.append((did, s))
        else:
            vnorm = _minmax(vec_scores)
            lnorm = _minmax(lex_scores)
            for did in all_ids:
                s = alpha * vnorm.get(did, 0.0) + (1.0 - alpha) * lnorm.get(did, 0.0)
                fused_pairs.append((did, s))

        fused_pairs.sort(key=lambda x: x[1], reverse=True)
        top_ids = [did for did, _ in fused_pairs[:k]]

        # -------- Attach metadata & write JSON --------
        def meta_get(did: int) -> str:
            return id2name.get(did) or id2name.get(int(did)) or id2name.get(str(did), "")

        results: List[dict] = []
        fused_map = dict(fused_pairs)

        for did in top_ids:
            chunkname = meta_get(did)
            filepath = chunk_to_real_path(chunkname, source_html_root=source_html_root)
            filetxt = chunk_to_real_txt(chunkname, source_html_root=source_txt_root)

            results.append(
                {
                    "doc_id": int(did),
                    "chunkname": chunkname,
                    "filename in txt": filetxt,
                    "file_path": filepath,
                    "scores": {
                        "vector": float(vec_scores.get(did, 0.0)),
                        "lexical": float(lex_scores.get(did, 0.0)),
                        "hybrid": float(fused_map.get(did, 0.0)),
                    },
                }
            )

        out_path = Path(ctx.cfg["paths"].get("hybrid_results_path", "./data/workspace/hybrid_results.json"))
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(
            json.dumps({"query": qtext, "results": results}, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        log.info("Hybrid top-%d written to %s", k, out_path)
        ctx.artifacts["hybrid_results_path"] = str(out_path)


---

Minimal config additions

Add this to your config (optional but recommended):

bm25:
  use_german_defaults: true       # set false if you provide your own stopwords
  stopwords: []                   # you can list custom stopwords here

paths:
  bm25_cache_file: "./data/workspace/bm25_cache.pkl"   # where the BM25 pickle lives

> The class still uses your existing hardcoded roots:

source_html_root = C:\Entwicklung\IAAEntwicklung\KI1\CIH\Help

source_txt_root  = C:\Entwicklung\IAAEntwicklung\KI1\upload\Daten\workspace\txt_files
Move these to config if you prefer.





---

Notes

The BM25 cache stores the tokenized corpus + mapping so subsequent runs are fast. Delete the pickle if you change txt files or stopwords.

If your vector_metadata.json omits some chunks that exist on disk, that’s fine—BM25 uses only the IDs present in the metadata so fusion lines up.

You can tune fusion by switching method to "rrf" in your config; your code already supports it.


If you want the tokenizer to stem German words, I can add a tiny optional stemmer (still dependency-free) or plug in snowballstemmer—just say the word.


Here you go—a small, robust caller that takes the payload you already built, sends it to your LLM endpoint, and prints the model’s answer. It also handles common errors (bad key, bad URL, model issues) without crashing.

import requests

def call_llm_and_print(api_url: str, api_key: str, payload: dict) -> str:
    """
    Sends the prepared chat payload to the LLM and prints the result.
    Returns the text content (or an error string) so you can also capture it.
    """
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    try:
        resp = requests.post(api_url, headers=headers, json=payload, timeout=90)
    except requests.exceptions.MissingSchema:
        msg = f"[URL ERROR] Ungültige API-URL: {api_url}"
        print(msg)
        return msg
    except requests.exceptions.ConnectionError as e:
        msg = f"[CONNECTION ERROR] Keine Verbindung zur API unter '{api_url}': {e}"
        print(msg)
        return msg
    except requests.exceptions.Timeout:
        msg = "[TIMEOUT] Die Anfrage an das LLM hat zu lange gedauert."
        print(msg)
        return msg
    except requests.exceptions.RequestException as e:
        msg = f"[REQUEST ERROR] {e}"
        print(msg)
        return msg

    # --- HTTP status handling ---
    if resp.status_code == 401:
        msg = "[AUTH ERROR] Ungültiger/abgelaufener API-Key (401)."
        print(msg)
        return msg
    if resp.status_code in (404, 405):
        msg = f"[URL/ENDPOINT ERROR] HTTP {resp.status_code}. Prüfe die API-URL oder Route."
        print(msg)
        return msg
    if resp.status_code == 400 and "model" in resp.text.lower():
        msg = "[MODEL ERROR] Ungültiger Modellname (400)."
        print(msg)
        return msg
    if resp.status_code != 200:
        msg = f"[HTTP ERROR] {resp.status_code}: {resp.text[:500]}"
        print(msg)
        return msg

    # --- Parse response content safely ---
    try:
        data = resp.json()
    except ValueError:
        msg = f"[PARSE ERROR] Keine gültige JSON-Antwort: {resp.text[:500]}"
        print(msg)
        return msg

    # Try common response shapes
    content = (
        data.get("choices", [{}])[0]
            .get("message", {})
            .get("content")
        or data.get("message", {}).get("content")
        or data.get("content")
        or ""
    ).strip()

    if not content:
        msg = "[LLM ERROR] Leere Antwort vom Modell."
        print(msg)
        return msg

    print("\n===== LLM Antwort =====\n")
    print(content)
    return content

Example usage (fits your previous builder)

# Build your strict payload first:
# payload = build_payload_strict_best_one(query, loaded_files, model_name="llama3.3:70b", top_k=10)

# Then call:
# result_text = call_llm_and_print(API_URL, API_KEY, payload)

Prints a clear error message for common failures.

Returns the text so you can log/store it in your pipeline.



# src/steps/hybrid_search_impls/CHybridSearchV1.py

from pathlib import Path
import json
from typing import Dict, List, Tuple
import os
import re

import faiss
import numpy as np

from whoosh import index as whoosh_index
from whoosh.scoring import BM25F
from whoosh import query as Q

from sentence_transformers import SentenceTransformer

# Project-local utilities
from src.steps.hybrid_search_impls.CHybridSearchABS import hybridSearch
from src.pipeline.Context import Context
from src.utils.logging import get_logger


# -----------------------------
# helpers
# -----------------------------

def _normalize2(v: np.ndarray) -> np.ndarray:
    v = v.astype("float32")
    n = np.linalg.norm(v, axis=-1, keepdims=True)
    n = np.maximum(n, 1e-12)
    return v / n


def _is_ivf(idx) -> bool:
    return isinstance(
        idx,
        (faiss.IndexIVFFlat, faiss.IndexIVFPQ, faiss.IndexIVFScalarQuantizer),
    )


def _minmax(scores: Dict[int, float]) -> Dict[int, float]:
    if not scores:
        return {}
    vals = list(scores.values())
    lo, hi = min(vals), max(vals)
    if hi - lo <= 1e-12:
        return {k: 1.0 for k in scores}
    return {k: (v - lo) / (hi - lo) for k, v in scores.items()}


def _zscore(scores: Dict[int, float]) -> Dict[int, float]:
    if not scores:
        return {}
    vals = np.array(list(scores.values()), dtype="float32")
    mu = float(vals.mean())
    sd = float(vals.std() + 1e-12)
    return {k: (v - mu) / sd for k, v in scores.items()}


def _rrf(rank: int, k: int = 60) -> float:
    return 1.0 / (k + rank)


# convert "content\strasse\profran_0001_chunk0004.txt" -> "content/strasse/profran_00001.htm"
_CHUNK_SUFFIX_RE = re.compile(r"_chunk\d{4}\.txt$", re.IGNORECASE)


def chunk_to_real_path(chunk_filename: str, source_html_root: str | None = None) -> str:
    """
    Map a chunk filename (as stored in vector_metadata.json) to the original .htm file.
    """
    rel = chunk_filename
    rel = _CHUNK_SUFFIX_RE.sub(".htm", rel)
    rel = rel.replace("\\", "/").replace("/", os.sep)
    return os.path.join(source_html_root, rel) if source_html_root else rel


def chunk_to_real_txt(chunk_filename: str, source_txt_root: str | None = None) -> str:
    """
    Map a chunk filename (as stored in vector_metadata.json) to the original .txt file.
    """
    rel = chunk_filename
    rel = _CHUNK_SUFFIX_RE.sub(".txt", rel)
    rel = rel.replace("\\", "/").replace("/", os.sep)
    return os.path.join(source_txt_root, rel) if source_txt_root else rel


def _norm_path(s: str) -> str:
    return s.replace("\\", "/")


# -----------------------------
# main class
# -----------------------------

class HybridSearchV1(hybridSearch):
    name = "hybridSearchV1"

    def run(self, ctx: Context) -> None:
        log = get_logger(self.name)

        # -------- Paths & config --------
        vroot = Path(ctx.cfg["paths"]["vector_dataset_dir"])
        lroot = Path(ctx.cfg["paths"]["lexical_index_dir"])

        index_path = vroot / "vector_index.faiss"
        meta_path = vroot / "vector_metadata.json"

        if not index_path.exists() or not meta_path.exists():
            log.error("Missing FAISS index or metadata in %s", vroot)
            return
        if not lroot.exists():
            log.error("Missing lexical index dir: %s", lroot)
            return

        qcfg = ctx.cfg.get("query", {}) or {}
        fusion = ctx.cfg.get("fusion", {}) or {}

        k = int(qcfg.get("k", 10))
        k_vec = int(qcfg.get("k_vec", 30))
        k_lex = int(qcfg.get("k_lex", 30))
        method = str(fusion.get("method", "minmax")).lower()
        alpha = float(fusion.get("alpha", 0.5))

        # Query text
        qtext = ctx.artifacts.get("corrected_query") or qcfg.get("text")
        if not qtext:
            log.error("No query text provided. Set artifacts['query_text'] or cfg.query.text")
            return

        # -------- Embedder (local) --------
        ecfg = ctx.cfg.get("embedding", {}) or {}
        model_name = ecfg.get("model_name", "")
        cache_folder = ecfg.get("cache_folder", "./data/models/st")
        model = SentenceTransformer(model_name, cache_folder=cache_folder)

        esp = ctx.cfg.get("es_prefixes", {}) or {}
        use_es = bool(esp.get("use", False))
        qprefix = esp.get("query_prefix", "query: ")

        # -------- Load metadata & build name<->id maps --------
        raw_meta = json.loads(Path(meta_path).read_text(encoding="utf-8"))
        # Supports:
        #   {"id_to_filename": {"0": "file_chunk0000.txt", ...}}
        # or flat dict {"0": "file_chunk0000.txt", ...}
        id2name = raw_meta.get("id_to_filename", raw_meta)
        # normalize to forward slashes for matching
        name2id = {_norm_path(v): int(k) for k, v in id2name.items()}

        def meta_get(did: int) -> str:
            return id2name.get(str(did)) or id2name.get(did) or ""

        # -------- FAISS search --------
        index = faiss.read_index(str(index_path))

        if _is_ivf(index):
            nprobe = int(ctx.cfg.get("faiss", {}).get("nprobe", 32))
            try:
                faiss.ParameterSpace().set_index_parameter(index, "nprobe", nprobe)
            except Exception:
                try:
                    index.nprobe = nprobe  # type: ignore[attr-defined]
                except Exception:
                    pass

        q_in = f"{qprefix}{qtext}" if use_es else qtext
        qv = model.encode([q_in], show_progress_bar=False)
        qv = _normalize2(np.asarray(qv, dtype="float32"))

        D, I = index.search(qv, k_vec)  # inner product on normalized vectors == cosine similarity
        vec_scores: Dict[int, float] = {}
        for doc_id, score in zip(I[0].tolist(), D[0].tolist()):
            if int(doc_id) < 0:
                continue
            vec_scores[int(doc_id)] = float(score)

        # -------- BM25 search (robust for sentences) --------
        ix = whoosh_index.open_dir(lroot)  # FileIndex is NOT a context manager
        with ix.searcher(weighting=BM25F()) as searcher:
            analyzer = ix.schema["text"].analyzer
            tokens = [tok.text for tok in analyzer(qtext) if tok.text]
            q = Q.Or([Q.Term("text", t) for t in tokens]) if tokens else Q.Every()

            hits = searcher.search(q, limit=k_lex)

            lex_scores: Dict[int, float] = {}
            for h in hits:
                # Prefer a stored FAISS id if present and valid
                did = h.get("doc_id")
                real_id = None
                if did is not None and str(did) in id2name:
                    real_id = int(did)
                else:
                    # Fallback: map via stored filename-like field
                    fields = searcher.stored_fields(h.docnum)
                    for key in ("chunkname", "filename", "path", "file", "name"):
                        val = fields.get(key)
                        if isinstance(val, str):
                            real_id = name2id.get(_norm_path(val))
                            if real_id is not None:
                                break

                if real_id is None:
                    # Could not map this Whoosh hit into FAISS id space -> skip
                    continue
                lex_scores[int(real_id)] = float(h.score)

        # -------- fusion --------
        all_ids = set(vec_scores) | set(lex_scores)
        fused_pairs: List[Tuple[int, float]] = []

        if method == "rrf":
            vec_sorted = sorted(vec_scores.items(), key=lambda x: x[1], reverse=True)
            lex_sorted = sorted(lex_scores.items(), key=lambda x: x[1], reverse=True)
            vec_rank = {doc_id: (r + 1) for r, (doc_id, _) in enumerate(vec_sorted)}
            lex_rank = {doc_id: (r + 1) for r, (doc_id, _) in enumerate(lex_sorted)}
            for did in all_ids:
                vr = vec_rank.get(did, len(vec_rank) + 1)
                lr = lex_rank.get(did, len(lex_rank) + 1)
                s = _rrf(vr) + _rrf(lr)
                fused_pairs.append((did, s))
        elif method == "zscore":
            vnorm = _zscore(vec_scores)
            lnorm = _zscore(lex_scores)
            for did in all_ids:
                s = alpha * vnorm.get(did, 0.0) + (1.0 - alpha) * lnorm.get(did, 0.0)
                fused_pairs.append((did, s))
        else:  # "minmax" default
            vnorm = _minmax(vec_scores)
            lnorm = _minmax(lex_scores)
            for did in all_ids:
                s = alpha * vnorm.get(did, 0.0) + (1.0 - alpha) * lnorm.get(did, 0.0)
                fused_pairs.append((did, s))

        fused_pairs.sort(key=lambda x: x[1], reverse=True)
        top_ids = [did for (did, _) in fused_pairs[:k]]

        # -------- Attach metadata & write JSON --------
        results = []
        fused_map = dict(fused_pairs)

        source_html_root = ctx.cfg["paths"].get("source_html_root", "")
        source_txt_root = ctx.cfg["paths"].get("source_txt_root", "")

        for did in top_ids:
            fname = meta_get(did)
            file_html = chunk_to_real_path(fname, source_html_root or None)
            file_txt = chunk_to_real_txt(fname, source_txt_root or None)

            results.append(
                {
                    "doc_id": int(did),
                    "chunkname": fname,
                    "filename in txt": str(Path(file_txt).name),
                    "file path": file_html,
                    "scores": {
                        "vector": float(vec_scores.get(did, 0.0)),
                        "lexical": float(lex_scores.get(did, 0.0)),
                        "hybrid": float(fused_map.get(did, 0.0)),
                    },
                }
            )

        out_path = Path(ctx.cfg["paths"].get("hybrid_results_path", "./data/workspace/hybrid_results.json"))
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(
            json.dumps({"query": qtext, "results": results}, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        log.info("Hybrid top %d written to %s", k, out_path)
        ctx.artifacts["hybrid_results_path"] = str(out_path)
you got it — here’s a single, complete script that searches your existing chunk .txt files with BM25 (keyword) + FAISS (semantic) and fuses the results.
No NLTK required (uses a fast regex tokenizer). Drop it in as hybrid_search_from_chunks.py.

# hybrid_search_from_chunks.py
# Hybrid retrieval over chunked .txt files (BM25 + FAISS), no NLTK dependency.

import os
import re
import glob
import argparse
from dataclasses import dataclass
from typing import List, Tuple, Dict, Iterable

import numpy as np
from tqdm import tqdm
from rank_bm25 import BM25Okapi
import faiss
from sentence_transformers import SentenceTransformer


# ----------------------------
# Tokenizer (no NLTK needed)
# ----------------------------
_token_re = re.compile(r"\b\w+\b", flags=re.UNICODE)

def simple_tokenize(text: str) -> List[str]:
    return _token_re.findall(text.lower())


# ----------------------------
# File utils
# ----------------------------
def read_txt_files(folder: str, recursive: bool = False, encoding: str = "utf-8") -> Tuple[List[str], List[str]]:
    """
    Load all .txt files under `folder`.
    Returns: (texts, filenames) aligned by index.
    """
    pattern = "**/*.txt" if recursive else "*.txt"
    files = sorted(glob.glob(os.path.join(folder, pattern), recursive=recursive))
    texts, names = [], []
    for fp in files:
        try:
            with open(fp, "r", encoding=encoding, errors="ignore") as f:
                txt = f.read().strip()
            if txt:
                texts.append(txt)
                names.append(fp)
        except Exception as e:
            print(f"[warn] Could not read {fp}: {e}")
    return texts, names


def batch(it: Iterable, n: int) -> Iterable[List]:
    buf = []
    for x in it:
        buf.append(x)
        if len(buf) == n:
            yield buf
            buf = []
    if buf:
        yield buf


# ----------------------------
# Core data structures
# ----------------------------
@dataclass
class ScoredItem:
    idx: int
    score: float


class HybridRetriever:
    def __init__(
        self,
        texts: List[str],
        names: List[str],
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        bm25_k1: float = 1.5,
        bm25_b: float = 0.75,
        use_cosine: bool = True,
        embed_batch_size: int = 128,
        show_progress: bool = True,
        ivf: int = 0,  # 0 = exact flat; >0 = IVF centroids count
    ):
        """
        texts: chunk texts (each .txt is one chunk)
        names: filenames aligned with `texts`
        ivf:   number of IVF centroids (0 uses exact IndexFlat*)
        """
        assert len(texts) == len(names) and len(texts) > 0, "texts and names must align and be non-empty"

        self.texts = texts
        self.names = names
        self.use_cosine = use_cosine

        # --- BM25 (sparse) ---
        if show_progress:
            print("Building BM25 index...")
        self.tokens = [simple_tokenize(t) for t in texts]
        self.bm25 = BM25Okapi(self.tokens, k1=bm25_k1, b=bm25_b)

        # --- Dense (SentenceTransformer + FAISS) ---
        if show_progress:
            print(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)

        if show_progress:
            print("Embedding chunks (dense vectors)...")
        embs = []
        for mini in tqdm(batch(texts, embed_batch_size),
                         total=(len(texts) + embed_batch_size - 1) // embed_batch_size,
                         disable=not show_progress):
            vecs = self.model.encode(
                mini,
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=embed_batch_size,
                normalize_embeddings=False,
            ).astype("float32")
            embs.append(vecs)
        embs = np.vstack(embs).astype("float32")
        self.embs = embs  # keep if you want to update the index later

        d = embs.shape[1]
        if self.use_cosine:
            # normalize to use inner product as cosine similarity
            faiss.normalize_L2(embs)
            metric = faiss.METRIC_INNER_PRODUCT
            flat = faiss.IndexFlatIP(d)
        else:
            metric = faiss.METRIC_L2
            flat = faiss.IndexFlatL2(d)

        if ivf and ivf > 0:
            if show_progress:
                print(f"Building IVF index with {ivf} centroids...")
            quantizer = flat
            index = faiss.IndexIVFFlat(quantizer, d, ivf, metric)
            if not index.is_trained:
                index.train(embs)
            index.add(embs)
            self.index = index
        else:
            if show_progress:
                print("Building exact flat index...")
            flat.add(embs)
            self.index = flat

        if show_progress:
            print(f"Indexed {len(texts)} chunks.")

    # ------------- helpers -------------
    @staticmethod
    def _minmax(scores: Dict[int, float]) -> Dict[int, float]:
        if not scores:
            return {}
        vals = np.fromiter(scores.values(), dtype="float32")
        vmin, vmax = float(vals.min()), float(vals.max())
        if vmax == vmin:
            return {k: 1.0 for k in scores}
        return {k: (v - vmin) / (vmax - vmin) for k, v in scores.items()}

    # ------------- retrieval -------------
    def _bm25_topk(self, query: str, k: int) -> List[ScoredItem]:
        q_tokens = simple_tokenize(query)
        scores = self.bm25.get_scores(q_tokens)  # shape (N,)
        k = min(k, len(scores))
        if k == len(scores):
            order = np.argsort(scores)[::-1]
        else:
            cand = np.argpartition(scores, -k)[-k:]
            order = cand[np.argsort(scores[cand])[::-1]]
        return [ScoredItem(int(i), float(scores[i])) for i in order[:k]]

    def _faiss_topk(self, query: str, k: int) -> List[ScoredItem]:
        q = self.model.encode([query], convert_to_numpy=True).astype("float32")
        if self.use_cosine:
            faiss.normalize_L2(q)
        k = min(k, len(self.texts))
        sims, idxs = self.index.search(q, k)  # returns (1,k)
        sims, idxs = sims[0], idxs[0]
        # if metric is L2, sims are negative distances only if we negated; but we used IP for cosine
        if not self.use_cosine:
            sims = -sims  # turn distances into similarity (higher is better)
        return [ScoredItem(int(i), float(sims[j])) for j, i in enumerate(idxs)]

    def _rrf(self, lists: List[List[ScoredItem]], k: int, rrf_k: float = 60.0) -> List[ScoredItem]:
        scores: Dict[int, float] = {}
        for lst in lists:
            for rank, it in enumerate(lst):
                scores[it.idx] = scores.get(it.idx, 0.0) + 1.0 / (rrf_k + rank + 1)
        out = [ScoredItem(i, s) for i, s in scores.items()]
        out.sort(key=lambda x: x.score, reverse=True)
        return out[:k]

    # ------------- public API -------------
    def search(
        self,
        query: str,
        k_bm25: int = 50,
        k_faiss: int = 50,
        k_final: int = 10,
        alpha: float = 0.5,
        use_rrf: bool = False,
    ) -> List[Tuple[str, float, str]]:
        """
        Returns a ranked list of (filename, fused_score, text).
        alpha: 0..1 weight for FAISS after min-max normalization.
        """
        if not query.strip():
            return []

        bm25 = self._bm25_topk(query, k_bm25)
        faiss_ = self._faiss_topk(query, k_faiss)

        if use_rrf:
            fused = self._rrf([bm25, faiss_], k_final)
            return [(self.names[it.idx], it.score, self.texts[it.idx]) for it in fused]

        # Weighted fusion (min–max normalize to comparable scales)
        sparse = {it.idx: it.score for it in bm25}
        dense  = {it.idx: it.score for it in faiss_}
        sparse_n = self._minmax(sparse)
        dense_n  = self._minmax(dense)

        all_ids = set(sparse_n) | set(dense_n)
        fused = []
        for i in all_ids:
            s = sparse_n.get(i, 0.0)
            d = dense_n.get(i, 0.0)
            fused.append(ScoredItem(i, (1 - alpha) * s + alpha * d))

        fused.sort(key=lambda x: x.score, reverse=True)
        fused = fused[:k_final]
        return [(self.names[it.idx], it.score, self.texts[it.idx]) for it in fused]


# ----------------------------
# CLI
# ----------------------------
def main():
    p = argparse.ArgumentParser(description="Hybrid search over chunk .txt files (BM25 + FAISS).")
    p.add_argument("--folder", required=True, help="Path to folder containing .txt chunk files.")
    p.add_argument("--recursive", action="store_true", help="Include subfolders.")
    p.add_argument("--model", default="sentence-transformers/all-MiniLM-L6-v2", help="SentenceTransformer model.")
    p.add_argument("--alpha", type=float, default=0.5, help="Fusion weight for FAISS (0=only BM25, 1=only FAISS).")
    p.add_argument("--k", type=int, default=5, help="Number of final results to show.")
    p.add_argument("--rrf", action="store_true", help="Use Reciprocal Rank Fusion instead of weighted fusion.")
    p.add_argument("--ivf", type=int, default=0, help="Use IVF with this many centroids (0 = exact flat).")
    args = p.parse_args()

    texts, names = read_txt_files(args.folder, recursive=args.recursive)
    if not texts:
        raise SystemExit("No .txt files found. Check the folder path or file extensions.")

    retriever = HybridRetriever(
        texts,
        names,
        model_name=args.model,
        ivf=args.ivf,
        show_progress=True,
    )

    print("\nHybrid search ready. Type your query (blank to exit):")
    while True:
        try:
            q = input("> ").strip()
        except (EOFError, KeyboardInterrupt):
            print()
            break
        if not q:
            break

        results = retriever.search(q, k_final=args.k, alpha=args.alpha, use_rrf=args.rrf)

        print("\nTop results:")
        for i, (fname, score, text) in enumerate(results, 1):
            base = os.path.basename(fname)
            preview = text.replace("\n", " ")[:160] + ("..." if len(text) > 160 else "")
            print(f"{i:>2}. {base} | score={score:.4f}")
            print(f"    {preview}")
        print()

if __name__ == "__main__":
    main()

How to run

pip install rank-bm25 faiss-cpu sentence-transformers numpy tqdm
python hybrid_search_from_chunks.py --folder /path/to/chunking_files --k 10 --alpha 0.6
# add --recursive if your chunks are in subfolders
# try IVF for big corpora:
# python hybrid_search_from_chunks.py --folder /path --ivf 1024

What it does

reads every .txt chunk → builds BM25 and FAISS indexes

query runs through both → results fused (weighted by --alpha or --rrf)

prints filename, fused score, preview


If you hit any errors, paste the first traceback line and I’ll zero in on it.


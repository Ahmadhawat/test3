import os
import glob
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Dict, Iterable, Optional

from tqdm import tqdm
from rank_bm25 import BM25Okapi
import faiss
from sentence_transformers import SentenceTransformer

import nltk
nltk.download('punkt', quiet=True)
from nltk.tokenize import word_tokenize


# ----------------------------
# Utilities
# ----------------------------
def read_txt_files(folder: str, recursive: bool = False, encoding_guess: str = "utf-8") -> Tuple[List[str], List[str]]:
    """
    Reads all .txt files under `folder`.
    Returns: (texts, filenames) aligned by index.
    """
    pattern = "**/*.txt" if recursive else "*.txt"
    files = sorted(glob.glob(os.path.join(folder, pattern), recursive=recursive))
    texts, names = [], []
    for fp in files:
        try:
            with open(fp, "r", encoding=encoding_guess, errors="ignore") as f:
                txt = f.read().strip()
            if txt:
                texts.append(txt)
                names.append(fp)
        except Exception as e:
            print(f"[warn] Could not read {fp}: {e}")
    return texts, names


def batch(iterable: Iterable, n: int) -> Iterable[List]:
    """Yield lists of size n from iterable."""
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) == n:
            yield buf
            buf = []
    if buf:
        yield buf


# ----------------------------
# Core data structures
# ----------------------------
@dataclass
class ScoredItem:
    idx: int
    score: float


class HybridRetriever:
    def __init__(
        self,
        texts: List[str],
        names: List[str],
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        bm25_k1: float = 1.5,
        bm25_b: float = 0.75,
        use_cosine: bool = True,
        embed_batch_size: int = 128,
        show_progress: bool = True,
    ):
        """
        texts: chunk texts (each .txt file is one chunk)
        names: filenames aligned with `texts`
        """
        assert len(texts) == len(names) and len(texts) > 0, "texts and names must align and be non-empty"

        self.texts = texts
        self.names = names

        # --- BM25 (sparse) ---
        if show_progress:
            print("Building BM25 index...")
        self.tokens = [word_tokenize(t.lower()) for t in texts]
        self.bm25 = BM25Okapi(self.tokens, k1=bm25_k1, b=bm25_b)

        # --- Dense (SentenceTransformer + FAISS) ---
        if show_progress:
            print(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)

        if show_progress:
            print("Embedding chunks (dense vectors)...")

        embs = []
        for mini in tqdm(batch(texts, embed_batch_size), disable=not show_progress, total=(len(texts) + embed_batch_size - 1)//embed_batch_size):
            vecs = self.model.encode(mini, show_progress_bar=False, convert_to_numpy=True, batch_size=embed_batch_size, normalize_embeddings=False)
            embs.append(vecs.astype("float32"))
        embs = np.vstack(embs).astype("float32")

        self.use_cosine = use_cosine
        if use_cosine:
            # Normalize for cosine similarity (then use inner product)
            faiss.normalize_L2(embs)
            self.index = faiss.IndexFlatIP(embs.shape[1])
        else:
            self.index = faiss.IndexFlatL2(embs.shape[1])

        self.index.add(embs)
        if show_progress:
            print(f"Indexed {len(texts)} chunks.")

    # ----------------------------
    # Internal helpers
    # ----------------------------
    @staticmethod
    def _minmax(scores: Dict[int, float]) -> Dict[int, float]:
        if not scores:
            return {}
        vals = np.fromiter(scores.values(), dtype="float32")
        vmin, vmax = float(vals.min()), float(vals.max())
        if vmax == vmin:
            return {k: 1.0 for k in scores}
        return {k: (v - vmin) / (vmax - vmin) for k, v in scores.items()}

    def _bm25_topk(self, query: str, k: int) -> List<ScoredItem]:
        q_tokens = word_tokenize(query.lower())
        scores = self.bm25.get_scores(q_tokens)
        if k >= len(scores):
            order = np.argsort(scores)[::-1]
        else:
            # partial sort then exact sort among top-k
            cand = np.argpartition(scores, -k)[-k:]
            order = cand[np.argsort(scores[cand])[::-1]]
        return [ScoredItem(int(i), float(scores[i])) for i in order[:k]]

    def _faiss_topk(self, query: str, k: int) -> List<ScoredItem]:
        q = self.model.encode([query], convert_to_numpy=True).astype("float32")
        if self.use_cosine:
            faiss.normalize_L2(q)
            sims, idxs = self.index.search(q, min(k, len(self.texts)))
            sims, idxs = sims[0], idxs[0]
            return [ScoredItem(int(i), float(sims[j])) for j, i in enumerate(idxs)]
        else:
            dists, idxs = self.index.search(q, min(k, len(self.texts)))
            dists, idxs = dists[0], idxs[0]
            sims = -dists  # convert L2 distance to similarity
            return [ScoredItem(int(i), float(sims[j])) for j, i in enumerate(idxs)]

    def _rrf(self, lists: List[List<ScoredItem]], k: int, rrf_k: float = 60.0) -> List<ScoredItem]:
        scores: Dict[int, float] = {}
        for lst in lists:
            for rank, it in enumerate(lst):
                scores[it.idx] = scores.get(it.idx, 0.0) + 1.0 / (rrf_k + rank + 1)
        out = [ScoredItem(i, s) for i, s in scores.items()]
        out.sort(key=lambda x: x.score, reverse=True)
        return out[:k]

    # ----------------------------
    # Public search API
    # ----------------------------
    def search(
        self,
        query: str,
        k_bm25: int = 50,
        k_faiss: int = 50,
        k_final: int = 10,
        alpha: float = 0.5,
        use_rrf: bool = False,
    ) -> List[Tuple[str, float, str]]:
        """
        Returns a ranked list of (filename, fused_score, text).

        alpha: weight for FAISS after min-max normalization in [0,1]
        use_rrf: if True, use Reciprocal Rank Fusion instead of weighted scores
        """
        bm25 = self._bm25_topk(query, k_bm25)
        faiss_ = self._faiss_topk(query, k_faiss)

        if use_rrf:
            fused = self._rrf([bm25, faiss_], k_final)
            return [(self.names[it.idx], it.score, self.texts[it.idx]) for it in fused]

        # Weighted fusion with min-max normalization
        sparse = {it.idx: it.score for it in bm25}
        dense = {it.idx: it.score for it in faiss_}
        sparse_n = self._minmax(sparse)
        dense_n = self._minmax(dense)
        all_ids = set(sparse_n) | set(dense_n)

        fused = []
        for i in all_ids:
            s = sparse_n.get(i, 0.0)
            d = dense_n.get(i, 0.0)
            fused.append(ScoredItem(i, (1 - alpha) * s + alpha * d))

        fused.sort(key=lambda x: x.score, reverse=True)
        fused = fused[:k_final]
        return [(self.names[it.idx], it.score, self.texts[it.idx]) for it in fused]


# ----------------------------
# CLI usage
# ----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Hybrid search over chunk .txt files (BM25 + FAISS).")
    parser.add_argument("--folder", required=True, help="Path to folder containing .txt chunk files.")
    parser.add_argument("--recursive", action="store_true", help="Search .txt files recursively.")
    parser.add_argument("--alpha", type=float, default=0.5, help="Weight for FAISS in [0,1].")
    parser.add_argument("--k", type=int, default=5, help="Number of final results.")
    parser.add_argument("--rrf", action="store_true", help="Use Reciprocal Rank Fusion instead of weighted.")
    args = parser.parse_args()

    texts, names = read_txt_files(args.folder, recursive=args.recursive)
    if not texts:
        raise SystemExit("No .txt files found. Check the folder path or file extensions.")

    retriever = HybridRetriever(texts, names)

    print("\nHybrid search ready. Type your query (or blank to exit):")
    while True:
        try:
            q = input("> ").strip()
        except (EOFError, KeyboardInterrupt):
            break
        if not q:
            break

        results = retriever.search(q, k_final=args.k, alpha=args.alpha, use_rrf=args.rrf)

        print("\nTop results:")
        for i, (fname, score, text) in enumerate(results, 1):
            preview = text.replace("\n", " ")[:160]
            if len(text) > 160:
                preview += "..."
            print(f"{i:>2}. {fname} | score={score:.4f}")
            print(f"    {preview}")
        print()


3) How to run

Basic (non-recursive, 5 results, 50/50 weighting):

python hybrid_search_from_chunks.py --folder /path/to/chunking_files --k 5 --alpha 0.5

Recursive (search subfolders too) and use RRF:

python hybrid_search_from_chunks.py --folder /path/to/chunking_files --recursive --k 10 --rrf
